# -*- coding: utf-8 -*-
"""Delphi_Prediction2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UdZDOjhm9tZRdhOpWMbe34qVZfxf-xDr
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import requests
import json
import re

def req(path,ele,class_tag,prop = 'class_'):
  r = requests.get(path)

  soup = BeautifulSoup(r.content, 'html.parser')
  # print(soup)
  # print(soup.find(ele,class_='{}'.format(class_tag)))
  s1 = eval('soup.find("{}",{}="{}")'.format(ele,prop,class_tag))
  return s1

def split_ret(s):
  s = s.split('\n')
  s = [st for st in s if len(st)>0]
  return s

def ret_data(s1,content,col,c,prev,f,other_head):
  for s in s1:
    if (other_head and s.name == 'p' and s.attrs and s['class'][0]==other_head) \
    or s.name == 'h4' or s.name == 'h3' or s.name =='h2' or s.name =='h1':
      if prev:
        content.append(c)
        c=[]
        prev =0
      if f:
        del col[-1]
      f=1
      col.append(s.text)
    if s.name =='p' and len(col):
      dum = split_ret(s.text)
      if len(dum):
        c.append(dum[0])
      # print(c)
      prev,f = 1,0
    if s.name =='ul':
      if len(c):
        dum = s.find_all('li')
        sub_txt = c[-1]
        del c[-1]
        for t in dum:
          # print(content)
          c.append(sub_txt+t.text)
      else:
        dum = s.find_all('li')
        for t in dum:
          c.append(split_ret(t.text)[0])
        content.append(c)
        c = []
      f=0

  if len(c):
    content.append(c)
  # break
  # for dum in content:
  #   print(dum)
  # print(col)
  l = min(len(content),len(col))
  data = pd.DataFrame(content[0:l],col[0:l])
  return data

def run_all(path,flag,ele = 'div',topic1 = 'text-content',prop2 = 'h2',\
            topic2 = 'article-content__wrapper',prop = 'class_',other_head = None):
  path = path
  # print("topic1 =====> ",topic1)
  s1 = req(path,ele,topic1,prop)
  # print(s1)
  content = []
  col = []
  c = []
  prev = 0
  f = 0
  if flag:
    s11 = req(path,ele,topic2,prop)
    # print("topic2 =====> ",topic2)
    col.append(split_ret(eval("s11.find('{}')".format(prop2)).text)[0])
    f=1
  data = ret_data(s1,content,col,c,prev,f,other_head)
  return data

def preprocess(dat,data):
  # print(dat.index)
  for i in dat:
    for d in dat[i]:
      if d==None:
        continue
      dummy = d.split('.')
      dummy = [t for t in dummy if len(t)>2]
      data = data+dummy
  # print(data)
  return data

from tqdm import tqdm

class MakeApiCall:

    def get_data(self,query,i):
        response = requests.get(f"{self.api}+{query}")
        if response.status_code == 200:
            return response.json()
        else:
            print(query,i)
            print(
                f"Hello person, there's a {response.status_code} error with your request")

    def get_user_data(self, api, parameters):
        response = requests.get(f"{api}", params=parameters)
        if response.status_code == 200:
            print("sucessfully fetched the data with parameters provided")
            self.formatted_print(response.json())
        else:
            print(
                f"Hello person, there's a {response.status_code} error with your request")

    def formatted_print(self, obj):
        text = json.dumps(obj, sort_keys=True, indent=4)
        print(text)

    def __init__(self, api):
        # self.get_data(api)

        parameters = {
            "username": "kedark"
        }
        self.api = api

file_path = r'/content/MEA_positive_new.csv'
df = pd.read_csv(file_path,encoding='unicode_escape')



dfn=df['sentences'].dropna()

pred_class=[]
pred_sen=[]

import requests
import json
# URL to make the GET request to
url = 'https://mosaic-api-frontdoor.apps.allenai.org/predict?action1='


#k=0
for j in tqdm(dfn):
    #k=k+1
    #if(k==378):
        #continue

    x=j.replace(' ','+')
    url1=url+x
    response = requests.get(url1)
    if response.status_code == 200:
        d=response.content.decode('utf-8')
        res = json.loads(d)
        pred_class.append(res['answer']['class'])
        pred_sen.append(res['answer']['text'])

    else:
        print('Failed to make GET request. Status code:', response.status_code)
        pred_class.append('NULL')
        pred_sen.append('NULL')

dat_india = pd.DataFrame(list(zip(list(df['sentences'].values),pred_class,pred_sen)),columns = ['etiquette','class','sentiment'])

dat_india.to_csv('Meanew_N_Delphi.csv')

from google.colab import drive
drive.mount('/content/drive')